{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca5a437",
   "metadata": {},
   "source": [
    "Assignment 2 - Regression and Classification Models\n",
    "\n",
    "1. Load the libraries - all in one for the whole assignment\n",
    "2. Load the data\n",
    "3. Inspect and clean the csv - I used the same data_preprocessor.py file from the previous assignment\n",
    "\n",
    "I googled A LOT, I used examples of functions that I found online (I added the links). ChatGPT was my troubleshooting companion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading libraries ###\n",
    "import data_preprocessor as dp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import pandas as pd\n",
    "\n",
    "### loading data ###\n",
    "messy_data = pd.read_csv(r'../Data/heart_disease_uci.csv', na_values=['', 'NA', 'Na', 'null', \"\"])\n",
    "working_data = messy_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec72bea7",
   "metadata": {},
   "source": [
    "Look into the data - used the data wrangler to get a better understanding of the data.\n",
    "I basically did the steps similar to the first assignmnent, using the already defined functions in data_preprocessor.py\n",
    "\n",
    "Steps:\n",
    "- drop duplicates\n",
    "- remove columns and rows missing > 25% data\n",
    "- as \"chol\" and \"num\" columns are targers, I deleted rows with missing values in \"chol\" (\"num\" didn;t have missing data)\n",
    "- I visualized outliers in numeric columns and then deleted them\n",
    "- I dropped 9 more rows instead of imputing. 1 from \"restecg\" and 8 from \"fbs\". Normally I would impute the values but both of those are cathegorical columns meaning imputing would either hit the target or not and the overall number of rows was very low.\n",
    "- I also looked for correlation but didn't find it.\n",
    "\n",
    "Please feel free to un-comment portions of code to double check my work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f001d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### cleaning the data ###\n",
    "working_data = working_data.drop_duplicates()\n",
    "working_data = dp.remove_columns_missing_data(working_data)\n",
    "working_data = dp.remove_rows_missing_data(working_data)\n",
    "# target columns: \"chol\" and \"num\"\n",
    "working_data = working_data.dropna(subset = \"chol\")\n",
    "\n",
    "# for column in [\"trestbps\",\"chol\",\"thalch\",\"oldpeak\"]:\n",
    "#     sns.boxplot(x=working_data[column])\n",
    "#     plt.title(f\"Boxplot of {column}\")\n",
    "#     plt.show()\n",
    "\n",
    "columns = [\"trestbps\",\"chol\",\"thalch\",\"oldpeak\"]\n",
    "working_data = dp.remove_outliers(working_data,columns)\n",
    "\n",
    "working_data = working_data.dropna(subset = \"fbs\") # just 8\n",
    "working_data = working_data.dropna(subset = \"restecg\") # just 1\n",
    "\n",
    "# print(working_data[[\"trestbps\",\"chol\",\"thalch\",\"oldpeak\",\"num\"]].corr())\n",
    "# no correlation found\n",
    "\n",
    "clean_data = working_data\n",
    "working_data.to_csv(r\"../Data/clean_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff85e0d",
   "metadata": {},
   "source": [
    "1. Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b1f9d",
   "metadata": {},
   "source": [
    "The next stop was splitting the data for training and testing at the standard 80-20. In the first model we're looking to predict cholesterol, y, based on everything else, x.\n",
    "Steps:\n",
    "- assign x and y values\n",
    "- convert categotical data into numeric. this is what google suggested cause apparently \"male\" is not a scalar\n",
    "- split data\n",
    "\n",
    "Standardizing data - normalize=True parameter didn't work. I used StandardScaler. \n",
    "(source: https://dataaspirant.com/elasticnet-regression-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df09f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### split data into training and test ###\n",
    "# define x and y \n",
    "X = clean_data.drop(columns='chol')\n",
    "y = clean_data['chol']\n",
    "\n",
    "# convert categorical columns to numeric\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# standardize data\n",
    "scaler = StandardScaler()\n",
    "scaled_x_train = scaler.fit_transform(X_train)\n",
    "scaled_x_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b45e6",
   "metadata": {},
   "source": [
    "Train the model - \n",
    "I looped the model through the pre-chosen alpha and l1_ratio values. I created the results dataframe to see what's going on - i needed this dataframe for chosing the best configuration.\n",
    "Then I chose to use hyperparameter grid to determine the best parameters.\n",
    "(source: https://dataaspirant.com/elasticnet-regression-python/)\n",
    "\n",
    "Choice of l1 rations: 0.1, 0.5, 0.9: almost entirely L1, evenly split, almost entirely L2 and alphas: I chose logarythmically spaced values to have the most information with the least amount of values.\n",
    "\n",
    "Print output:\n",
    "Best alpha: 1.0\n",
    "Best l1_ratio: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859567ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "alphas = [0.1, 1, 10]\n",
    "l1_ratios = [0.1, 0.5, 0.9]\n",
    "results=[]\n",
    "\n",
    "for alpha in alphas:\n",
    "    for l1_ratio in l1_ratios:\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "        model.fit(scaled_x_train, y_train)\n",
    "        y_pred = model.predict(scaled_x_test)\n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        results.append((alpha, l1_ratio, rmse, r2))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"alpha\", \"l1_ratio\", \"RMSE\", \"R2\"])\n",
    "results_df.to_csv(r\"../Data/results_df.csv\", index=False)\n",
    "\n",
    "# Using hyperparameter grid to determine best alpha and l1 ratio\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 1, 10],\n",
    "    'l1_ratio': [0.1, 0.5, 0.9],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_search.fit(scaled_x_train, y_train)\n",
    "\n",
    "print('Best alpha:', grid_search.best_estimator_.alpha)\n",
    "print('Best l1_ratio:', grid_search.best_estimator_.l1_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db009014",
   "metadata": {},
   "source": [
    "Determining the optimal configuration. \n",
    "\n",
    "I'm leaving this part for your enjoyment. After 2h of trying to reinvent the wheel, I realised GridSearchCV actually gave me the answer to this question. \n",
    "\n",
    "Steps:\n",
    "- minmax normalize R2 and RMSE (used the function from data_preprocessor.py)\n",
    "- because higher R2 is better and lower RMSE is better, I made a column with 1-RMSE to invert the column. (this way max sum would be 2)\n",
    "- sum up R2 and 1-RMSE\n",
    "- I stopped the insanity here and just read the highest value from the csv.\n",
    "\n",
    "best parameters (config):\n",
    "alpha: 1.0\n",
    "l1 ratio: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e78d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df[\"R2_scaled\"] = dp.normalize_data(results_df[[\"R2\"]].copy(), [\"R2\"])[\"R2\"]\n",
    "# results_df.to_csv(r\"../Data/results_df.csv\", index=False)\n",
    "\n",
    "results_df[\"RMSE_scaled\"] = dp.normalize_data(results_df[[\"RMSE\"]].copy(), [\"RMSE\"])[\"RMSE\"]\n",
    "results_df[\"RMSE_scaled_reversed\"] = 1- dp.normalize_data(results_df[[\"RMSE\"]].copy(), [\"RMSE\"])[\"RMSE\"]\n",
    "\n",
    "results_df[\"final_score\"] = results_df[\"R2_scaled\"] + results_df[\"RMSE_scaled_reversed\"]\n",
    "\n",
    "results_df.to_csv(r\"../Data/results_df.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7bac8c",
   "metadata": {},
   "source": [
    "Visualization!\n",
    "Heatmaps are cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ccbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot R2 heatmap\n",
    "r2_pivot = results_df.pivot(index=\"alpha\", columns=\"l1_ratio\", values=\"R2\")\n",
    "sns.heatmap(r2_pivot, annot=True)\n",
    "plt.title(\"R² Heatmap (ElasticNet)\")\n",
    "plt.xlabel(\"l1_ratio\")\n",
    "plt.ylabel(\"alpha\")\n",
    "plt.show()\n",
    "\n",
    "# Plot RMSE heatmap\n",
    "rmse_pivot = results_df.pivot(index=\"alpha\", columns=\"l1_ratio\", values=\"RMSE\")\n",
    "sns.heatmap(rmse_pivot, annot=True)\n",
    "plt.title(\"RMSE Heatmap (ElasticNet)\")\n",
    "plt.xlabel(\"l1_ratio\")\n",
    "plt.ylabel(\"alpha\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b72d3",
   "metadata": {},
   "source": [
    "2. Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd21d3f",
   "metadata": {},
   "source": [
    "I used the same clean_data, divided and standardized features the same way as above. Only this time the target (y) was predicting the presence or absence of heart disease (column \"num\"). The other difference is because having a disease is a 0 or 1 kind of thing, I binarized the outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d0666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### split data into training and test ###\n",
    "# define x and y \n",
    "X = clean_data.drop(columns='num')\n",
    "y = clean_data['num']\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# binarize the outcome\n",
    "y = (y > 0).astype(int) \n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# standardize data\n",
    "scaler = StandardScaler()\n",
    "scaled_x_train = scaler.fit_transform(X_train)\n",
    "scaled_x_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e036698",
   "metadata": {},
   "source": [
    "Train logistic regression model\n",
    "(source: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html mixed with lab3) \n",
    "I used the list of solvers and penalties from the link above too. Not all are compatible so I used try-except while looping through the possibilities.\n",
    "Then I summed up the evaluation metrics to fund the best one. \n",
    "I identified the best combo by opening the csv.\n",
    "\n",
    "The best combo: saga, l1\n",
    "2nd best: liblinear,l1\n",
    "Note: accuracy and f1 were the same for all combinations, the rest had very minor differences.\n",
    "\n",
    "The best scores are to save the data of the best config to make the figure.\n",
    "\n",
    "If I did it again, I'd put the figure function under the loop to have more figures/data to compare, not just the figure of the best one. That goes for kNN too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811704db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LogisticRegression\n",
    "solvers = [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]\n",
    "penalties = ['l1', 'l2']\n",
    "results = []\n",
    "best_score = 0\n",
    "best_log_model = None\n",
    "log_best_fpr = log_best_tpr = log_best_precision = log_best_recall = None\n",
    "\n",
    "for solver in solvers:\n",
    "    for penalty in penalties:\n",
    "        try:\n",
    "            model = LogisticRegression(solver=solver, penalty=penalty, max_iter=10000)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_scores = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "            # evaluation metrics:\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "            auroc = auc(fpr, tpr) \n",
    "            precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "            auprc = average_precision_score(y_test, y_scores)\n",
    "\n",
    "            score = accuracy + f1 + auroc + auprc\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_log_model = model\n",
    "                log_best_fpr = fpr\n",
    "                log_best_tpr = tpr\n",
    "                log_best_precision = precision\n",
    "                log_best_recall = recall\n",
    "\n",
    "            #print(f\"pair: {solver} - {penalty}\")\n",
    "            results.append((solver, penalty, accuracy, f1, auroc, auprc, fpr, tpr, precision, recall))\n",
    "        except Exception as e:\n",
    "            #print(f\"Skipped: solver={solver}, penalty={penalty} — {e}\")\n",
    "            continue\n",
    "\n",
    "results_summary = pd.DataFrame(results, columns=[\"solver\", \"penalty\", \"accuracy\", \"f1\", \"auroc\", \"auprc\", \"_\", \"_\", \"_\", \"_\"])\n",
    "results_summary = results_summary.drop(columns=[\"_\", \"_\", \"_\", \"_\"])\n",
    "# results_summary.to_csv(r\"../Data/results_summary.csv\", index=False)\n",
    "\n",
    "# find the best config:\n",
    "results_summary[\"sum\"] = results_summary[[\"accuracy\", \"f1\", \"auroc\", \"auprc\"]].sum(axis=1)\n",
    "results_summary.to_csv(r\"../Data/results_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c940d",
   "metadata": {},
   "source": [
    "k-NN\n",
    "I looped the k-NN over the values mentioned in the assignment [1,5,10]\n",
    "For the code, I adjusted the code from lab4.\n",
    "Overall the idea was the same as above - looping throgh the k values and saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 5, 10]\n",
    "best_knn_score = 0\n",
    "best_knn_model = None\n",
    "knn_best_fpr = knn_best_tpr = knn_best_precision = knn_best_recall = None\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='manhattan')\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_scores_knn = knn.predict_proba(X_test_scaled)[:, 1]\n",
    "    y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "    f1 = f1_score(y_test, y_pred_knn)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_scores_knn)\n",
    "    auroc = auc(fpr, tpr)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_scores_knn)\n",
    "    auprc = average_precision_score(y_test, y_scores_knn)\n",
    "\n",
    "    score = accuracy + f1 + auroc + auprc\n",
    "    if score > best_knn_score:\n",
    "        best_knn_score = score\n",
    "        best_knn_model = knn\n",
    "        knn_best_fpr, knn_best_tpr = fpr, tpr\n",
    "        knn_best_precision, knn_best_recall = precision, recall\n",
    "    \n",
    "    knn_results.append((k, accuracy, f1, auroc, auprc))\n",
    "\n",
    "knn_results_summary = pd.DataFrame(knn_results, columns=[\"k\", \"accuracy\", \"f1\", \"auroc\", \"auprc\"])\n",
    "knn_results_summary[\"sum\"] = knn_results_summary[[\"accuracy\", \"f1\", \"auroc\", \"auprc\"]].sum(axis=1)\n",
    "knn_results_summary.to_csv(r\"../Data/knn_results_summary.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78547752",
   "metadata": {},
   "source": [
    "All the plots.\n",
    "The function for the plots was also taken from lab4 and is defined in data_preprocessor.py file.\n",
    "First regression, then k-NN\n",
    "Both plots were used for the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b485afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best logistic regression model curves\n",
    "log_auprc = average_precision_score(y_test, best_log_model.predict_proba(X_test_scaled)[:, 1])\n",
    "dp.plot_curves(log_best_tpr, log_best_fpr, auc(log_best_fpr, log_best_tpr), log_best_precision, log_best_recall, log_auprc, 'Logistic Regression', minority_class=y.mean())\n",
    "\n",
    "# Best k-NN model curves\n",
    "knn_auprc = average_precision_score(y_test, best_knn_model.predict_proba(X_test_scaled)[:, 1])\n",
    "dp.plot_curves(knn_best_tpr, knn_best_fpr, auc(knn_best_fpr, knn_best_tpr), knn_best_precision, knn_best_recall, knn_auprc, 'k-NN', minority_class=y.mean())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
