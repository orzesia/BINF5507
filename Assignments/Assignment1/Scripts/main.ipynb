{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 1 - Joanna Orzechowska\n",
    "\n",
    "This code has 3 parts:\n",
    "\n",
    "Admin tasks: import libraries and load the data and look at the data\n",
    "\n",
    "Preprocessing: cleaning, imputing and normalizing data\n",
    "\n",
    "Model: assessing the accuracy of the model - spoiler alert: 0.855\n",
    "\n",
    "I will put the important information before the code (including the results)\n",
    "\n",
    "AI disclosure: ChatGPT was used for troubleshooting, clarifying a few concepts (ex IQR), help with strings describing functions and readability of the readme file.\n",
    "\n",
    "I did not however use AI to fix grammar and readability of this document. I'm sorry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Admin tasks\n",
    "- importing all necessary libraries\n",
    "- loading the dataset\n",
    "- initial look into the data\n",
    "I used Data Wrangler extension to visualize data. Below is the data wrangler summary:\n",
    "\n",
    "Data shape      1,196 rows x 28 columns\n",
    "\n",
    "Rows with missing values        1,148 (96.0%)\n",
    "\n",
    "Duplicate rows      0 (0.0%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import data_preprocessor as dp\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "messy_data = pd.read_csv('../Data/messy_data.csv', na_values=['', 'NA', 'Na', 'null', \"\"])\n",
    "working_data = messy_data.copy()\n",
    "\n",
    "# look into the messy data\n",
    "print(messy_data.head())\n",
    "print(messy_data.info())\n",
    "print(messy_data.describe())\n",
    "print(working_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Cleaning the data\n",
    "\n",
    "This step is divided into multiple substeps, all results are saved into their own csv:\n",
    "\n",
    "1. remove duplicates        -       cleaning_step1.csv\n",
    "2. delete columns with >= 25% of missing values     -       cleaning_step2.csv\n",
    "3. remove rows with >= 25% of missing values        -       cleaning_step3.csv\n",
    "4. delete redundant features        -       cleaning_step4.csv\n",
    "5. delete rows with missing value in the target column      -       cleaning_step5.csv\n",
    "6. remove outliers      -       cleaning_step_outliers.csv (plot twist)\n",
    "7. impute missing values        -       cleaning_step6.csv\n",
    "8. normalize data       -       clean_data.csv\n",
    "\n",
    "Some of the steps just neede to be there, no matter the order, others like removing outliers I wanted to do on the cleanest possible data before imputing and normalizing.\n",
    "\n",
    "The cells need to be ran in an order, no skipping as they build on each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Removing duplicates. \n",
    "According to Data Wrangler, there were no duplicate rows. After I ran the function, the stats were the same as the messy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 remove duplicates\n",
    "\n",
    "working_data = dp.remove_duplicates(working_data)\n",
    "working_data.to_csv(r\"../Data/cleaning_step1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Removing columns with >=25% missing data\n",
    "\n",
    "The model needs data to predict data. There's no point keeping columns with 25+% of values missing.\n",
    "\n",
    "Stats after:\n",
    "\n",
    "Data shape 1,196 rows x 23 columns\n",
    "\n",
    "Rows with missing values    752 (62.9%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 remove columns with missing data\n",
    "\n",
    "working_data = dp.remove_columns_missing_data(working_data)\n",
    "working_data.to_csv(r\"../Data/cleaning_step2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Removing rows with >=25% missing data\n",
    "\n",
    "In retrospect, I should have probably done this step after deleting redundant features but instead I calculated the fraction of rows with missing 25+% of data to know how much data I'm deleting (6.18%). \n",
    "\n",
    "Stats after:\n",
    "\n",
    "Data shape  1,122 rows x 23 columns\n",
    "\n",
    "Rows with missing values    678 (60.4%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.1 fraction of rows that have 25%+ of missing data\n",
    "\n",
    "fraction = dp.count_missing_data_rows(working_data)\n",
    "print(f\"Fraction of rows with >=25% missing data fraction: {round(fraction,2)}\")\n",
    "\n",
    "# 2.3.2 remove rows with 25+% missing values\n",
    "working_data = dp.remove_rows_missing_data(working_data)\n",
    "working_data.to_csv(r\"../Data/cleaning_step3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Deleting redundant features\n",
    "\n",
    "First I looked into the data. 2 possible clusters became apparent: columns o,p and q, and columns r,u and y. I used to corr() function to compare only the columns I chose. They all showed correlation of 0.98 and above. I deleted all but r and p as those columns had the least % of missing values and most distinct values as per Data Wrangler.\n",
    "\n",
    "Then I decided to compare all numerical columns left and saved the resulting data frame as \"x.csv\". I went rogue and used conditional formatting in excel to see if any correlation was above 0.9 and I discovered 2 other correlations (both above 0.98) and deleted colums c and n.\n",
    "\n",
    "Stats after:\n",
    "\n",
    "Data shape  1,122 rows x 17 columns\n",
    "\n",
    "Rows with missing values    458 (40.8%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.1 exploration into redundant features\n",
    "print(working_data[[\"o\",\"p\",\"q\"]].corr())\n",
    "print(working_data[[\"r\",\"u\",\"y\"]].corr())\n",
    "# leaving r and p\n",
    "x=working_data[[\"b\",\"c\",\"f\",\"h\",\"k\",\"l\",\"n\",\"p\",\"r\",\"t\",\"v\",\"w\"]].corr()\n",
    "x.to_csv(r\"../Data/x.csv\", index=False)\n",
    "\n",
    "# 4.2 deleting redundant features\n",
    "working_data = working_data.drop(columns =[\"o\",\"q\",\"u\",\"y\",\"c\",\"n\"])\n",
    "working_data.to_csv(r\"../Data/cleaning_step4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Delete rows with missing target\n",
    "\n",
    "I first checked how rows have a many missing target (0.23). And then I deleted them as imputing values would assign either 1 or 0, skewing learning.\n",
    "\n",
    "Stats after:\n",
    "\n",
    "Data shape     865 rows x 17 columns\n",
    "\n",
    "Rows with missing values    201 (23.2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 fraction of rows with missing target\n",
    "fraction = working_data[\"target\"].isnull().mean()\n",
    "print(f\"Fraction of rows with missing target data: {round(fraction,2)}\")\n",
    "\n",
    "# 5.2 delete rows with missing target\n",
    "working_data = working_data.dropna(subset = \"target\")\n",
    "working_data.to_csv(r\"../Data/cleaning_step5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6 Outliers\n",
    "\n",
    "First I made box plots of each numerical column to see which columns have outliers. I used IQR method to deleted those rows. I wanted to do this step before imputing and especially normalizing so the outliers don't skew the mean/median/mode.\n",
    "\n",
    "Stats after:\n",
    "\n",
    "Data shape  690 rows x 17 columns\n",
    "\n",
    "Rows with missing values    98 (14.2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6.1 visualization\n",
    "for column in [\"b\",\"f\",\"k\",\"l\",\"p\",\"r\",\"v\",\"w\",\"h\",\"t\",\"target\",\"a\",\"i\"]:\n",
    "    sns.boxplot(x=working_data[column])\n",
    "    plt.title(f\"Boxplot of {column}\")\n",
    "    plt.show()\n",
    "\n",
    "# 2.6.2 remove outliers (IQR)\n",
    "columns = [\"b\",\"f\",\"k\",\"p\",\"l\",\"v\",\"w\"]\n",
    "working_data = dp.remove_outliers(working_data,columns)\n",
    "working_data.to_csv(r\"../Data/cleaning_step_outliers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7 Imputing missing values\n",
    "\n",
    "I used visualization from Data Wrangler to see the data distribution from each column. Based on distribution, I manually divided the colums into suitable methods: mean for normal distribution, median for skewed distribution and mode for discrete values.\n",
    "\n",
    "Stats after:\n",
    "\n",
    "Data shape      690 rows x 17 columns\n",
    "\n",
    "Rows with missing values    0 (0.0%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.7 impute missing values\n",
    "mean_columns = [\"b\",\"f\",\"k\",\"l\",\"p\",\"r\",\"v\",\"w\"]\n",
    "median_columns = [\"h\",\"t\"]\n",
    "mode_columns = [\"target\",\"a\",\"i\"]\n",
    "working_data = dp.impute_missing_values(working_data, mean_columns, median_columns, mode_columns)\n",
    "working_data.to_csv(r\"../Data/cleaning_step6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.8 Normalize the data\n",
    "\n",
    "I chose the MinMax method.\n",
    "\n",
    "Stats after are same as the step before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.8 normalize the data\n",
    "numeric_columns = mean_columns+median_columns\n",
    "clean_data = dp.normalize_data(working_data,numeric_columns)\n",
    "clean_data.to_csv(r\"../Data/clean_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Use the model\n",
    "\n",
    "Use the model to calculate it's accuracy and compare messy data (0.8) to clean data (0.855). The accuracy increased by 5.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train and evaluate the model\n",
    "model_results = dp.simple_model(messy_data)\n",
    "print(model_results)\n",
    "model_results = dp.simple_model(clean_data)\n",
    "print(model_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
